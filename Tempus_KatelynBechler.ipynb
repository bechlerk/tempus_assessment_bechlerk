{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000094af-835d-4c15-b68e-02e6a4f9aeb2",
   "metadata": {},
   "source": [
    "# Tempus AI Coding Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5cfa3-b2bb-49d6-81ac-825b6505090c",
   "metadata": {},
   "source": [
    "## Bioinformatics Scientist, Pipeline Development - Katelyn Bechler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4177afa9-22a4-42d4-8f43-5e7dfb40f8d5",
   "metadata": {},
   "source": [
    "### 1. Design a Scalable Twitter Clone in the Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c17ea-7429-4c33-919d-717c3b426106",
   "metadata": {},
   "source": [
    "1. Define MVP core functionality and features that must be supported - what are the core components that should exist in order for a \"Twitter\" to function in the cloud?\n",
    "    - Posts: write posts, add tags (mentions), add hashtags, like posts, unlike posts, repost, comment on posts\n",
    "    - Community: follow, un-follow, view followers list, view those you follow list, private messaging, search for users\n",
    "    - Technical: support for multi-media posts, flagging unsafe posts, ability to get live data/feeds, algorithmic newsfeed, notifications, search posts, username/password login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190fb47-b951-4e42-9d85-a48b97752bf4",
   "metadata": {},
   "source": [
    "2. Required cloud resources: I would select AWS as a cloud resource to build and deploy this MVP, but I acknowledge that Microsoft Azure or GCP could also be used. With AWS, I would use AWS EC2 instances to support the backend since then we can scale the number of EC2 instances based on traffic and number of users. For storage, I would use Amazon Redshift (PostgreSQL or another relational database) to store user data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6739d-bed3-495c-9e6d-622c719af25f",
   "metadata": {},
   "source": [
    "3. Scaling strategy: To help with scaling, I would add configurations that leverage an elastic load balancer to distribute user requests as evenly as possible across EC2 instances and leverage an application load balancer to then route the users to different EC2 instances. For storage, I might suggest to use Amazon S3 flat files to store large media like images and videos that are associated with posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def47d0-a552-477d-8cd9-3586b7ef8c58",
   "metadata": {},
   "source": [
    "4. Additional features and considerations: Additional features that might be added are ideas to help personalize a users' feed, provide following recommendations/post recommendations, highlight trending hashtagas. These features, as well as most personalized features would require using novel machine learning or AI methods - these would inherently require significant additional compute power since most of this work would be deep learning. Other considerations are for added security, especially as the world gets more technology savvy and cyber attacks are getting more realistic, it is crucial to ensure there is proper and robust security of users and their data. Lastly, as the app is developed, it may be useful to consider integrations with other data platforms to create a more wholistic user experience. These integrations could rqeuire a lot of scrunity from both a compute and security perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f1f817-249b-4ed5-9e38-ed6bfc9ea72d",
   "metadata": {},
   "source": [
    "5. Key questions to keep in mind while scaling and developing:\n",
    "    - How does the system handle peak user times? I.e. elections, breaking news, etc.\n",
    "    - What information needs to be real-time vs. information that can have time delays?\n",
    "    - How does the system handle monitoring, logging, failures, handling errors?\n",
    "    - What does user privacy look like?\n",
    "    - Are there user permissions, authentications, or authorization for certain data?\n",
    "    - How to handle authentications or authorizations?\n",
    "    - How does scaling impact costs?\n",
    "    - How does scaling impact speed?\n",
    "    - How do we monitor user behavior?\n",
    "    - How do we maintain a safe and secure platform?\n",
    "    - How do we ensure content is appriopriate and safe?\n",
    "    - How can be enourage integration with other data platforms?\n",
    "    - How do we handle user history?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722a86d-1ea6-4525-9da2-9e46942ccef8",
   "metadata": {},
   "source": [
    "### 2. Write a Dockerized pipeline in Nextflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9db40-a261-4684-9e80-3f2d01b77a66",
   "metadata": {},
   "source": [
    "Requirements: Take a stock data set, slice it by a relevant variable, fit a simple regression model on each slice, and then combine these models in a single table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9dc57-d6a8-4668-8ca3-2bac67270b68",
   "metadata": {},
   "source": [
    "The below is an EDA to explore the dataset and task in python, prior to integrating with Nextflow. See 'Problem2' on git."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b0e66d-1315-4c77-8bfa-6f4474df0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries.\n",
    "import pandas as pd\n",
    "from palmerpenguins import load_penguins\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b8bdf-637c-4a71-822c-f6c03556a1b4",
   "metadata": {},
   "source": [
    "#### 1. Load and slice dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c74c4-b7a2-45bc-8c55-3213325b3a6c",
   "metadata": {},
   "source": [
    "Palmer Penguins dataset: https://pypi.org/project/palmerpenguins/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ad994b-bef2-4d83-9e6b-f52ced1575ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and slice dataset by a relevant variable.\n",
    "penguins = load_penguins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e488bfe1-89d1-40ba-89f9-40b5cfbc3b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>male</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>female</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  year  \n",
       "0       3750.0    male  2007  \n",
       "1       3800.0  female  2007  \n",
       "2       3250.0  female  2007  \n",
       "3          NaN     NaN  2007  \n",
       "4       3450.0  female  2007  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6d5eb3-953e-4be7-96d9-2adde66eaf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "bill_length_mm        2\n",
       "bill_depth_mm         2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  11\n",
       "year                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1fcb040-8e57-49fe-800f-e545e44eac86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "Adelie       152\n",
       "Gentoo       124\n",
       "Chinstrap     68\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice dataset by species.\n",
    "penguins.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9d217c-5d9d-492a-b128-202fb867d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to run regression model on each slice of data.\n",
    "def fit_regression(sliced_data):\n",
    "    # Segment to features and target and drop NAs for linear regression.\n",
    "    sliced_data = sliced_data[['bill_length_mm', 'flipper_length_mm']]\n",
    "    sliced_data_na_rm = sliced_data.dropna()\n",
    "\n",
    "    # Define features and target.\n",
    "    X = sliced_data_na_rm[['bill_length_mm']]\n",
    "    # Drop rows with missing values.\n",
    "    y = sliced_data_na_rm['flipper_length_mm']\n",
    "\n",
    "    # Fit a regression model.\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X, y)\n",
    "\n",
    "    # Save the model as a pickle file.\n",
    "    joblib.dump(lr_model, f\"model_{species}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc861e4e-a675-4efb-8e53-1c0445de702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the dataset by spceis and save each sliced dataset.\n",
    "for species in penguins.species.unique():\n",
    "    sliced_data = penguins[penguins['species'] == species] \n",
    "    sliced_data.to_csv(f\"sliced_{species}.csv\", index = False)\n",
    "    fit_regression(sliced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff5c7676-2309-493b-92c6-e1fe84600311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sliced data.\n",
    "#sliced_data = pd.read_csv(\"${slice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23450a0a-34ff-4ddd-961a-da6c1bbad297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all models.\n",
    "models_combined = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7039eca2-a811-4d01-b5c9-d77ea85effdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each model and summarize its coefficients.\n",
    "for model_name in os.listdir(\".\"):\n",
    "    if model_name.startswith(\"model_\") and model_name.endswith(\".pkl\"):\n",
    "        model = joblib.load(model_name)\n",
    "        coefficients = model.coef_[0]\n",
    "        intercept = model.intercept_\n",
    "        species = model_name.replace(\"model_\", \"\").replace(\".pkl\", \"\")\n",
    "        models_combined.append({\"species\": species, \"intercept\": intercept, \"coefficient\": coefficients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f18222-e0f3-447d-bc41-035bb8d3f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the combined models into a DataFrame and save it.\n",
    "models_combined_df = pd.DataFrame(models_combined)\n",
    "models_combined_df.to_csv(\"models_combined.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a870b27-92c3-4892-bd70-152a095ccb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>intercept</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinstrap</td>\n",
       "      <td>146.635840</td>\n",
       "      <td>1.007246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>158.924442</td>\n",
       "      <td>0.799899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>151.096041</td>\n",
       "      <td>1.391246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     species   intercept  coefficient\n",
       "0  Chinstrap  146.635840     1.007246\n",
       "1     Adelie  158.924442     0.799899\n",
       "2     Gentoo  151.096041     1.391246"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1035cfc-3454-4389-af1b-4079b80cf4f2",
   "metadata": {},
   "source": [
    "### 3. Describe a Python package that you use regularly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475da359-6373-40f6-a854-da5d7253f078",
   "metadata": {},
   "source": [
    "Though this might be a big one to tackle, I'm going to talk about the *pandas* package - there is a lot to uncover here, but I want to highlight the main features, limitations, and areas for growth for this package.\n",
    "\n",
    "**Import classes/methods/functions**\n",
    "- Classes: DataFrame, Series, Index\n",
    "- Methods: .apply(), .map(), .dropna(), .sort_values(), .query()\n",
    "- Functions: working with csvs (read/write), bringing data together (merge, concat), simple analysis (pivot_table, groupby)\n",
    "\n",
    "Pandas is also extremely useful because it integrates well with other packages (Numpy, Matplotlib, Scikit-learn, Dask, Pytorch, TensorFlow, BeautifulSoup). Pandas is built on top of Numpy and also integrates very well with Matplotlib and Seaborn, which are plotting packages. At a high level, pandas is exceptional at manipulating datasets in clean, interpretable ways and carrying this forward in future tasks.\n",
    "\n",
    "Some limitations of the pandas packages are that it is not as efficient for large datasets because it mostly processes in-memory and on a single-node. This could be improved by looking for ways to process tasks across CPUS using frameworks like Apache Spark or to perform operations in parallel using methods like apply.\n",
    "\n",
    "Another limitation that has come up a lot for me personally is how pandas handles NA data. Pandas syntax can be fairly cumbersome/might not even exist (another limitation of pandas in general) to address all missing value issues, especially when we want to leverage downstream methods. For example, LinearRegression from sklearn can not take NA values and so it would be awesome if we could incorporate specialized functions that can create cleaner datasets in pandas resolving missing values through various methods, depending on user needs. If I were to propose working on this effort, I would create an advanced function for handling missing values based on custom parameters. A user could select to address NAs by column mean imputation, column mode imputation, column mediation imputation, KNN imputation, logistic regression imputation, removing all rows with any NA values, or encode missing values as a feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecda81-2106-4083-a42c-ba3da932f9d1",
   "metadata": {},
   "source": [
    "### 4. Part A: SQL/GBQ Challenge 1: Analyzing Genomic Data for Disease Association"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf208d-cee5-4d5e-9ba6-50a92567a115",
   "metadata": {},
   "source": [
    "See 'Problem4' on git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf19b6-33a1-473b-a433-dbe487574587",
   "metadata": {},
   "source": [
    "### 4. Part B: SQL/GBQ Challenge 2: Analyzing Genomic Data for Gene Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d503-8832-42f5-9373-dfafa280f072",
   "metadata": {},
   "source": [
    "See 'Problem4' on git."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
